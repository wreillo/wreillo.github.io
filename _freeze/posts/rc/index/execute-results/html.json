{
  "hash": "8c09d68f9bf6f69b71c0318a0f4de50c",
  "result": {
    "markdown": "---\ntitle: \"Research Computing Organizations\"\ndate: \"2023-09-12\"\ncategories: [tables]\nimage: \"featured.gif\"\nimage-alt: \"DT with Research Computing Organizations on GitHub\"\n---\n\n\n## Research Computing Organizations\n\nSimple R script to pull GitHub profiles matching for *research computing* in the organization name\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlibrary('rlist')\nlibrary('dplyr',quietly = TRUE)\n```\n\n::: {.cell-output .cell-output-stderr}\n```\n\nAttaching package: 'dplyr'\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:stats':\n\n    filter, lag\n```\n:::\n\n::: {.cell-output .cell-output-stderr}\n```\nThe following objects are masked from 'package:base':\n\n    intersect, setdiff, setequal, union\n```\n:::\n\n```{.r .cell-code}\nlibrary('rvest')\nlibrary('xml2')\nlibrary('DT')\n```\n:::\n\n::: {.cell}\n\n```{.r .cell-code}\n## Search Research Computing organizations\nrc.org <- \"https://api.github.com/search/users?q=research+computing+in:name+type:org&type=Users&per_page=100&page=%d\" %>%\n  sprintf(1:2) %>% list.load(\"json\") %>% list.ungroup(level = 3) %>%\n  .[names(.) %in% 'login'] %>% as.character\n\n# Scrape Function\n#   The resulted on n=130 \"Research Computing\" organizations. \n#   To waive the GH API rate limit, github_scrape(...) was used\ngithub_scrape <- function(org_names){\n\n  URL <- paste0(\"https://github.com/\", org_names)\n  edata <- read_html(URL)\n  body_nodes <- edata %>%\n    html_node('body') %>%\n    html_children()\n  \n  # LOCATION\n  LOCATION <- body_nodes %>% \n    xml_find_all(\"//span[contains(@itemprop, 'location')]\") %>% \n    html_text()\n  \n  # Organization website\n  WEBSITE <- body_nodes %>% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %>% \n    html_text() %>% tibble::enframe() %>% \n    filter(!startsWith(value,'@')) %>% \n    select(value) %>% as.character()\n  \n  # Organization Name\n  NAME <- body_nodes %>% \n    xml_find_all(\"//h1[contains(@class, 'h2 lh-condensed')]\") %>% \n    html_text() %>%\n    gsub(pattern = \"\\\\s+\",replacement = \" \") %>% \n    stringr::str_trim()\n  \n  # Twitter handle\n  TWITTER <- body_nodes %>% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %>% \n    html_text() %>% tibble::enframe() %>% \n    filter(startsWith(value,'@')) %>% \n    select(value) %>% as.character()\n  \n  # Metadata vector\n  VEC <- c(github=org_names,name=NAME, website=WEBSITE,twitter=TWITTER)\n  return(VEC)\n}\n\n# Do call\nrc.metadata <- do.call(rbind,lapply(rc.org, FUN=github_scrape))\n\n# Load scraped metadata\nmeta.df <- as.data.frame(rc.metadata)\nmeta.df <- meta.df[order(meta.df$github),]\n\n# Add GitHub hyperlinks\nghURL <- paste0(\"https://github.com/\", meta.df$github)\nmeta.df$github <-paste0(\"<a href='\",ghURL,\"'\",\n                        ' target=\\\"_blank\\\">',\n                        meta.df$github,\"</a>\")\n# Remove NA's\nmeta.df[meta.df == \"character(0)\"] <- NA\n\n# Reorder columns\nmeta.df <- subset(meta.df, select=c('github','name','twitter','website'))\n\n# Convert to DT\nmeta.dt <- DT::datatable(meta.df,escape = F,rownames = F,\n                         options = list(pageLength = 100,\n                                        autoWidth = TRUE,\n                                        fixedColumns = list(leftColumns = 0)))\nmeta.dt\n```\n:::\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {},
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}