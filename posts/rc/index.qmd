---
title: "Research Computing Organizations"
date: "2023-09-12"
categories: [tables]
image: "featured.gif"
image-alt: "DT with Research Computing Organizations on GitHub"
---

## Research Computing Organizations

*Research Computing* profiles on GitHub - manipulating html files with rvest:: and displaying results with DT::datatable

## Research Computing GitHub Profiles - Interactive Table using DT::datatable

```{r, echo = FALSE}
library('rlist')
suppressMessages(library('dplyr'))
library('rvest')
library('xml2')
library('DT')

## Search for RC Organizations
rc.org <- "https://api.github.com/search/users?q=research+computing+in:name+type:org&type=Users&per_page=100&page=%d" %>%
  sprintf(1:2) %>% list.load("json") %>% list.ungroup(level = 3) %>%
  .[names(.) %in% 'login'] %>% as.character

# Scrape Function
#   in-house github_scrape(...) func to skip the API rate limits
github_scrape <- function(org_names){

  URL <- paste0("https://github.com/", org_names)
  edata <- read_html(URL)
  body_nodes <- edata %>%
    html_node('body') %>%
    html_children()
  
  # LOCATION
  LOCATION <- body_nodes %>% 
    xml_find_all("//span[contains(@itemprop, 'location')]") %>% 
    html_text()
  
  # Organization website
  WEBSITE <- body_nodes %>% 
    xml_find_all("//a[contains(@itemprop, 'url')]") %>% 
    html_text() %>% tibble::enframe() %>% 
    filter(!startsWith(value,'@')) %>% 
    select(value) %>% as.character()
  
  # Organization Name
  NAME <- body_nodes %>% 
    xml_find_all("//h1[contains(@class, 'h2 lh-condensed')]") %>% 
    html_text() %>%
    gsub(pattern = "\\s+",replacement = " ") %>% 
    stringr::str_trim()
  
  # Twitter handle
  TWITTER <- body_nodes %>% 
    xml_find_all("//a[contains(@itemprop, 'url')]") %>% 
    html_text() %>% tibble::enframe() %>% 
    filter(startsWith(value,'@')) %>% 
    select(value) %>% as.character()
  
  # Metadata vector
  VEC <- c(github=org_names,name=NAME, website=WEBSITE,twitter=TWITTER)
  return(VEC)
}

# Do call
rc.metadata <- do.call(rbind,lapply(rc.org, FUN=github_scrape))

# Load scraped metadata
meta.df <- as.data.frame(rc.metadata)
meta.df <- meta.df[order(meta.df$github),]

# Add GitHub hyperlinks
ghURL <- paste0("https://github.com/", meta.df$github)
meta.df$github <-paste0("<a href='",ghURL,"'",
                        ' target=\"_blank\">',
                        meta.df$github,"</a>")
# Remove NA's
meta.df[meta.df == "character(0)"] <- NA

# Reorder columns
meta.df <- subset(meta.df, select=c('github','name','twitter','website'))

# Convert to DT
meta.dt <- DT::datatable(meta.df,escape = F,rownames = F,
                         options = list(pageLength = 100,
                                        autoWidth = TRUE,
                                        fixedColumns = list(leftColumns = 0)))
meta.dt
```

### R Script to Pull Profiles

```{r, eval = FALSE}
# Load R packages
library('rlist')
suppressMessages(library('dplyr'))
library('rvest')
library('xml2')
library('DT')

## Search Research Computing organizations
rc.org <- "https://api.github.com/search/users?q=research+computing+in:name+type:org&type=Users&per_page=100&page=%d" %>%
  sprintf(1:2) %>% list.load("json") %>% list.ungroup(level = 3) %>%
  .[names(.) %in% 'login'] %>% as.character

# Scrape Function
#   The resulted on n=130 "Research Computing" organizations. 
#   To waive the GH API rate limit, github_scrape(...) was used
github_scrape <- function(org_names){

  URL <- paste0("https://github.com/", org_names)
  edata <- read_html(URL)
  body_nodes <- edata %>%
    html_node('body') %>%
    html_children()
  
  # LOCATION
  LOCATION <- body_nodes %>% 
    xml_find_all("//span[contains(@itemprop, 'location')]") %>% 
    html_text()
  
  # Organization website
  WEBSITE <- body_nodes %>% 
    xml_find_all("//a[contains(@itemprop, 'url')]") %>% 
    html_text() %>% tibble::enframe() %>% 
    filter(!startsWith(value,'@')) %>% 
    select(value) %>% as.character()
  
  # Organization Name
  NAME <- body_nodes %>% 
    xml_find_all("//h1[contains(@class, 'h2 lh-condensed')]") %>% 
    html_text() %>%
    gsub(pattern = "\\s+",replacement = " ") %>% 
    stringr::str_trim()
  
  # Twitter handle
  TWITTER <- body_nodes %>% 
    xml_find_all("//a[contains(@itemprop, 'url')]") %>% 
    html_text() %>% tibble::enframe() %>% 
    filter(startsWith(value,'@')) %>% 
    select(value) %>% as.character()
  
  # Metadata vector
  VEC <- c(github=org_names,name=NAME, website=WEBSITE,twitter=TWITTER)
  return(VEC)
}

# Do call
rc.metadata <- do.call(rbind,lapply(rc.org, FUN=github_scrape))

# Load scraped metadata
meta.df <- as.data.frame(rc.metadata)
meta.df <- meta.df[order(meta.df$github),]

# Add GitHub hyperlinks
ghURL <- paste0("https://github.com/", meta.df$github)
meta.df$github <-paste0("<a href='",ghURL,"'",
                        ' target=\"_blank\">',
                        meta.df$github,"</a>")
# Remove NA's
meta.df[meta.df == "character(0)"] <- NA

# Reorder columns
meta.df <- subset(meta.df, select=c('github','name','twitter','website'))

# Convert to DT
meta.dt <- DT::datatable(meta.df,escape = F,rownames = F,
                         options = list(pageLength = 100,
                                        autoWidth = TRUE,
                                        fixedColumns = list(leftColumns = 0)))
meta.dt
```
