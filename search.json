[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Behind the keyboard ‚å®Ô∏è",
    "section": "",
    "text": "An space to explore new ideas, concepts, and share it with others. The website layout is adapted from the quarto-tip-a-day github page.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nHalloween meets Shiny - Reviewer #2\n\n\n\n\n\n\nNov 1, 2024\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nOpenStreetMap on R + Quarto\n\n\n\n\n\n\nSep 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Computing Organizations\n\n\n\n\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/halloween24/index.html",
    "href": "posts/halloween24/index.html",
    "title": "Halloween meets Shiny - Reviewer #2",
    "section": "",
    "text": "Figure 1: Reviewer #2 - scary mode [on]\n\n\nThis is the story of how a simple Shiny app became part of my costume for the Halloween TG at Scripps Oceanography.\nI decided to shave my head a few days before Halloween. That same day, when my wife came home, she stared at me for a moment and said, ‚ÄúYou look like a monk. You could go as Avatar for Halloween!‚Äù I didn‚Äôt say anything, but the idea sounded pretty cool! We had a bunch of work-related events on Thursday, the 31st, but on Friday, I joined her and other friends at TG.\nI pulled everything together to be The Scientist Monk, but the weather (ü•∂) didn‚Äôt cooperate. So, I needed something last-minute. Then, an intrusive thought struck: grad school prepared you for last-minute solutions - it‚Äôs time to Shine-y! Two minutes later, I had written an AI prompt to generate a few sentences inspired by the infamous Reviewer #2, embedded them in a super simple Shiny spinner, and headed over to TG (Fig. 1).\nLIVE DEMO\n\nlibrary(shiny)\nlibrary(shinyWidgets)\n\n# Define the sentences vector\nsentences &lt;- c(\n  \"The methodology lacks sufficient rigor and detail.\",\n  \"The experimental design does not adequately support the conclusions drawn.\",\n  \"The statistical analysis is insufficient or inappropriate for the data presented.\",\n  \"The sample size is too small to reach statistically significant conclusions.\",\n  \"There is a lack of control experiments, which undermines the validity of the findings.\",\n  \"The study lacks novelty and does not significantly advance the field.\",\n  \"The research question or hypothesis is not clearly stated or justified.\",\n  \"The study‚Äôs findings replicate existing literature without offering new insights.\",\n  \"The scope of the study is too narrow to be of interest to a broader audience.\",\n  \"The data provided are insufficient to support the conclusions.\",\n  \"The results are poorly presented or unclear.\",\n  \"The figures and tables are inadequate or do not add value to the manuscript.\",\n  \"There are inconsistencies in the data presented and the claims made.\",\n  \"The literature review is incomplete, overlooking important previous work.\",\n  \"The authors have not contextualized their findings within the existing body of research.\",\n  \"Relevant and recent studies are missing, which weakens the foundation of the research.\",\n  \"The manuscript lacks clarity and organization, making it difficult to follow.\",\n  \"The writing is overly technical, hindering comprehension for a broader audience.\",\n  \"There are numerous grammatical and typographical errors throughout the paper.\",\n  \"The abstract does not accurately reflect the content of the manuscript.\",\n  \"The conclusions are overstated and not fully supported by the data.\",\n  \"The authors overinterpret the significance of their findings.\",\n  \"The limitations of the study are not adequately discussed.\",\n  \"The manuscript does not meet the standards of this journal in terms of scope or quality.\",\n  \"The study does not offer sufficient impact to be of interest to the journal‚Äôs audience.\"\n  \n)\n\n# Define UI for application\nui &lt;- fluidPage(\n  titlePanel(\"Spin\"),\n  sidebarLayout(\n    sidebarPanel(\n      actionButton(\"spin\", \"Spin the Wheel!\", icon = icon(\"rotate\")),\n      tags$br(),\n      pickerInput(\n        \"sentencePicker\", \n        \"Your Sentence:\", \n        choices = sentences, \n        selected = sentences[1],\n        width = \"100%\"\n      )\n    ),\n    mainPanel(\n      textOutput(\"displaySentence\"),\n      br(),\n      tags$style(type=\"text/css\",\n                 \"  #displaySentence { \n                       font-size: 24px;\n                       font-family: Arial, sans-serif;\n                       text-align: center;\n                       margin-top: 20px;\n                     }\")\n    )\n  )\n)\n\n# Define server logic\nserver &lt;- function(input, output, session) {\n  \n  # Observer for spin button\n  observeEvent(input$spin, {\n    # Update pickerInput with a randomly selected sentence\n    updatePickerInput(session, \"sentencePicker\", selected = sample(sentences, 1))\n  })\n  \n  # Display selected sentence\n  output$displaySentence &lt;- renderText({\n    input$sentencePicker\n  })\n}\n\n# Run the application \nshinyApp(ui = ui, server = server)"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Behind the keyboard ‚å®Ô∏è",
    "section": "",
    "text": "W. Rodriguez\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n    \n      \n        William G. Rodriguez-Reillo Ph.D.\n        Data Scientist: Needle Finder & Consultant\n        \n        \n          \n          I'm a Research Computing Consultant with a Ph.D. in Organismic and Evolutionary Biology and a passion for bridging science and advanced computing. I specialize in supporting biomedical research through High Performance Computing (HPC), secure data environments, and AI workflow integration.\n          \n          With hands-on experience in genomics, containerized software deployment, virtualized infrastructure, and researcher training, I thrive at the intersection of science and scalable infrastructure. My goal is to empower researchers with the tools, platforms, and skills needed to advance discovery.\n          \n          Let‚Äôs connect if you‚Äôre working at the crossroads of science and technology!"
  },
  {
    "objectID": "posts/leaflet/index.html",
    "href": "posts/leaflet/index.html",
    "title": "OpenStreetMap on R + Quarto",
    "section": "",
    "text": "Create a map widget using the leaflet package on R\n\nlibrary(leaflet)\n\nm &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=-66.74041348869325, lat=18.468307266376055, popup=\"Mi Alma Materüê∫\")"
  },
  {
    "objectID": "posts/leaflet/index.html#leaflet-widget",
    "href": "posts/leaflet/index.html#leaflet-widget",
    "title": "OpenStreetMap on R + Quarto",
    "section": "",
    "text": "Create a map widget using the leaflet package on R\n\nlibrary(leaflet)\n\nm &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=-66.74041348869325, lat=18.468307266376055, popup=\"Mi Alma Materüê∫\")"
  },
  {
    "objectID": "posts/leaflet/index.html#mi-alma-mater",
    "href": "posts/leaflet/index.html#mi-alma-mater",
    "title": "OpenStreetMap on R + Quarto",
    "section": "Mi alma mater",
    "text": "Mi alma mater"
  },
  {
    "objectID": "posts/rc/index.html",
    "href": "posts/rc/index.html",
    "title": "Research Computing Organizations",
    "section": "",
    "text": "Simple R script to pull GitHub profiles matching for research computing in the organization name\n\nlibrary('rlist')\nsuppressMessages(library('dplyr'))\nlibrary('rvest')\nlibrary('xml2')\nlibrary('DT')\n\n\n## Search Research Computing organizations\nrc.org &lt;- \"https://api.github.com/search/users?q=research+computing+in:name+type:org&type=Users&per_page=100&page=%d\" %&gt;%\n  sprintf(1:2) %&gt;% list.load(\"json\") %&gt;% list.ungroup(level = 3) %&gt;%\n  .[names(.) %in% 'login'] %&gt;% as.character\n\n# Scrape Function\n#   The resulted on n=130 \"Research Computing\" organizations. \n#   To waive the GH API rate limit, github_scrape(...) was used\ngithub_scrape &lt;- function(org_names){\n\n  URL &lt;- paste0(\"https://github.com/\", org_names)\n  edata &lt;- read_html(URL)\n  body_nodes &lt;- edata %&gt;%\n    html_node('body') %&gt;%\n    html_children()\n  \n  # LOCATION\n  LOCATION &lt;- body_nodes %&gt;% \n    xml_find_all(\"//span[contains(@itemprop, 'location')]\") %&gt;% \n    html_text()\n  \n  # Organization website\n  WEBSITE &lt;- body_nodes %&gt;% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %&gt;% \n    html_text() %&gt;% tibble::enframe() %&gt;% \n    filter(!startsWith(value,'@')) %&gt;% \n    select(value) %&gt;% as.character()\n  \n  # Organization Name\n  NAME &lt;- body_nodes %&gt;% \n    xml_find_all(\"//h1[contains(@class, 'h2 lh-condensed')]\") %&gt;% \n    html_text() %&gt;%\n    gsub(pattern = \"\\\\s+\",replacement = \" \") %&gt;% \n    stringr::str_trim()\n  \n  # Twitter handle\n  TWITTER &lt;- body_nodes %&gt;% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %&gt;% \n    html_text() %&gt;% tibble::enframe() %&gt;% \n    filter(startsWith(value,'@')) %&gt;% \n    select(value) %&gt;% as.character()\n  \n  # Metadata vector\n  VEC &lt;- c(github=org_names,name=NAME, website=WEBSITE,twitter=TWITTER)\n  return(VEC)\n}\n\n# Do call\nrc.metadata &lt;- do.call(rbind,lapply(rc.org, FUN=github_scrape))\n\n# Load scraped metadata\nmeta.df &lt;- as.data.frame(rc.metadata)\nmeta.df &lt;- meta.df[order(meta.df$github),]\n\n# Add GitHub hyperlinks\nghURL &lt;- paste0(\"https://github.com/\", meta.df$github)\nmeta.df$github &lt;-paste0(\"&lt;a href='\",ghURL,\"'\",\n                        ' target=\\\"_blank\\\"&gt;',\n                        meta.df$github,\"&lt;/a&gt;\")\n# Remove NA's\nmeta.df[meta.df == \"character(0)\"] &lt;- NA\n\n# Reorder columns\nmeta.df &lt;- subset(meta.df, select=c('github','name','twitter','website'))\n\n# Convert to DT\nmeta.dt &lt;- DT::datatable(meta.df,escape = F,rownames = F,\n                         options = list(pageLength = 100,\n                                        autoWidth = TRUE,\n                                        fixedColumns = list(leftColumns = 0)))\nmeta.dt"
  },
  {
    "objectID": "posts/rc/index.html#research-computing-organizations",
    "href": "posts/rc/index.html#research-computing-organizations",
    "title": "Research Computing Organizations",
    "section": "",
    "text": "Simple R script to pull GitHub profiles matching for research computing in the organization name\n\nlibrary('rlist')\nsuppressMessages(library('dplyr'))\nlibrary('rvest')\nlibrary('xml2')\nlibrary('DT')\n\n\n## Search Research Computing organizations\nrc.org &lt;- \"https://api.github.com/search/users?q=research+computing+in:name+type:org&type=Users&per_page=100&page=%d\" %&gt;%\n  sprintf(1:2) %&gt;% list.load(\"json\") %&gt;% list.ungroup(level = 3) %&gt;%\n  .[names(.) %in% 'login'] %&gt;% as.character\n\n# Scrape Function\n#   The resulted on n=130 \"Research Computing\" organizations. \n#   To waive the GH API rate limit, github_scrape(...) was used\ngithub_scrape &lt;- function(org_names){\n\n  URL &lt;- paste0(\"https://github.com/\", org_names)\n  edata &lt;- read_html(URL)\n  body_nodes &lt;- edata %&gt;%\n    html_node('body') %&gt;%\n    html_children()\n  \n  # LOCATION\n  LOCATION &lt;- body_nodes %&gt;% \n    xml_find_all(\"//span[contains(@itemprop, 'location')]\") %&gt;% \n    html_text()\n  \n  # Organization website\n  WEBSITE &lt;- body_nodes %&gt;% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %&gt;% \n    html_text() %&gt;% tibble::enframe() %&gt;% \n    filter(!startsWith(value,'@')) %&gt;% \n    select(value) %&gt;% as.character()\n  \n  # Organization Name\n  NAME &lt;- body_nodes %&gt;% \n    xml_find_all(\"//h1[contains(@class, 'h2 lh-condensed')]\") %&gt;% \n    html_text() %&gt;%\n    gsub(pattern = \"\\\\s+\",replacement = \" \") %&gt;% \n    stringr::str_trim()\n  \n  # Twitter handle\n  TWITTER &lt;- body_nodes %&gt;% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %&gt;% \n    html_text() %&gt;% tibble::enframe() %&gt;% \n    filter(startsWith(value,'@')) %&gt;% \n    select(value) %&gt;% as.character()\n  \n  # Metadata vector\n  VEC &lt;- c(github=org_names,name=NAME, website=WEBSITE,twitter=TWITTER)\n  return(VEC)\n}\n\n# Do call\nrc.metadata &lt;- do.call(rbind,lapply(rc.org, FUN=github_scrape))\n\n# Load scraped metadata\nmeta.df &lt;- as.data.frame(rc.metadata)\nmeta.df &lt;- meta.df[order(meta.df$github),]\n\n# Add GitHub hyperlinks\nghURL &lt;- paste0(\"https://github.com/\", meta.df$github)\nmeta.df$github &lt;-paste0(\"&lt;a href='\",ghURL,\"'\",\n                        ' target=\\\"_blank\\\"&gt;',\n                        meta.df$github,\"&lt;/a&gt;\")\n# Remove NA's\nmeta.df[meta.df == \"character(0)\"] &lt;- NA\n\n# Reorder columns\nmeta.df &lt;- subset(meta.df, select=c('github','name','twitter','website'))\n\n# Convert to DT\nmeta.dt &lt;- DT::datatable(meta.df,escape = F,rownames = F,\n                         options = list(pageLength = 100,\n                                        autoWidth = TRUE,\n                                        fixedColumns = list(leftColumns = 0)))\nmeta.dt"
  }
]