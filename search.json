[
  {
    "objectID": "posts.html",
    "href": "posts.html",
    "title": "Behind the keyboard ‚å®Ô∏è",
    "section": "",
    "text": "An space to explore new ideas, concepts, and share it with others. The website layout is adapted from the quarto-tip-a-day github page.\n\n\n\n\n\n\n   \n     \n     \n       Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n    \n      \n      \n    \n\n\n\n\n\n\n\n\n\n\nOpenStreetMap on R + Quarto\n\n\n\n\n\n\nSep 16, 2023\n\n\n\n\n\n\n\n\n\n\n\n\n\n\nResearch Computing Organizations\n\n\n\n\n\n\nSep 12, 2023\n\n\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "posts/rc/index.html",
    "href": "posts/rc/index.html",
    "title": "Research Computing Organizations",
    "section": "",
    "text": "Simple R script to pull GitHub profiles matching for research computing in the organization name\n\nlibrary('rlist')\nsuppressMessages(library('dplyr'))\nlibrary('rvest')\nlibrary('xml2')\nlibrary('DT')\n\n\n## Search Research Computing organizations\nrc.org &lt;- \"https://api.github.com/search/users?q=research+computing+in:name+type:org&type=Users&per_page=100&page=%d\" %&gt;%\n  sprintf(1:2) %&gt;% list.load(\"json\") %&gt;% list.ungroup(level = 3) %&gt;%\n  .[names(.) %in% 'login'] %&gt;% as.character\n\n# Scrape Function\n#   The resulted on n=130 \"Research Computing\" organizations. \n#   To waive the GH API rate limit, github_scrape(...) was used\ngithub_scrape &lt;- function(org_names){\n\n  URL &lt;- paste0(\"https://github.com/\", org_names)\n  edata &lt;- read_html(URL)\n  body_nodes &lt;- edata %&gt;%\n    html_node('body') %&gt;%\n    html_children()\n  \n  # LOCATION\n  LOCATION &lt;- body_nodes %&gt;% \n    xml_find_all(\"//span[contains(@itemprop, 'location')]\") %&gt;% \n    html_text()\n  \n  # Organization website\n  WEBSITE &lt;- body_nodes %&gt;% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %&gt;% \n    html_text() %&gt;% tibble::enframe() %&gt;% \n    filter(!startsWith(value,'@')) %&gt;% \n    select(value) %&gt;% as.character()\n  \n  # Organization Name\n  NAME &lt;- body_nodes %&gt;% \n    xml_find_all(\"//h1[contains(@class, 'h2 lh-condensed')]\") %&gt;% \n    html_text() %&gt;%\n    gsub(pattern = \"\\\\s+\",replacement = \" \") %&gt;% \n    stringr::str_trim()\n  \n  # Twitter handle\n  TWITTER &lt;- body_nodes %&gt;% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %&gt;% \n    html_text() %&gt;% tibble::enframe() %&gt;% \n    filter(startsWith(value,'@')) %&gt;% \n    select(value) %&gt;% as.character()\n  \n  # Metadata vector\n  VEC &lt;- c(github=org_names,name=NAME, website=WEBSITE,twitter=TWITTER)\n  return(VEC)\n}\n\n# Do call\nrc.metadata &lt;- do.call(rbind,lapply(rc.org, FUN=github_scrape))\n\n# Load scraped metadata\nmeta.df &lt;- as.data.frame(rc.metadata)\nmeta.df &lt;- meta.df[order(meta.df$github),]\n\n# Add GitHub hyperlinks\nghURL &lt;- paste0(\"https://github.com/\", meta.df$github)\nmeta.df$github &lt;-paste0(\"&lt;a href='\",ghURL,\"'\",\n                        ' target=\\\"_blank\\\"&gt;',\n                        meta.df$github,\"&lt;/a&gt;\")\n# Remove NA's\nmeta.df[meta.df == \"character(0)\"] &lt;- NA\n\n# Reorder columns\nmeta.df &lt;- subset(meta.df, select=c('github','name','twitter','website'))\n\n# Convert to DT\nmeta.dt &lt;- DT::datatable(meta.df,escape = F,rownames = F,\n                         options = list(pageLength = 100,\n                                        autoWidth = TRUE,\n                                        fixedColumns = list(leftColumns = 0)))\nmeta.dt"
  },
  {
    "objectID": "posts/rc/index.html#research-computing-organizations",
    "href": "posts/rc/index.html#research-computing-organizations",
    "title": "Research Computing Organizations",
    "section": "",
    "text": "Simple R script to pull GitHub profiles matching for research computing in the organization name\n\nlibrary('rlist')\nsuppressMessages(library('dplyr'))\nlibrary('rvest')\nlibrary('xml2')\nlibrary('DT')\n\n\n## Search Research Computing organizations\nrc.org &lt;- \"https://api.github.com/search/users?q=research+computing+in:name+type:org&type=Users&per_page=100&page=%d\" %&gt;%\n  sprintf(1:2) %&gt;% list.load(\"json\") %&gt;% list.ungroup(level = 3) %&gt;%\n  .[names(.) %in% 'login'] %&gt;% as.character\n\n# Scrape Function\n#   The resulted on n=130 \"Research Computing\" organizations. \n#   To waive the GH API rate limit, github_scrape(...) was used\ngithub_scrape &lt;- function(org_names){\n\n  URL &lt;- paste0(\"https://github.com/\", org_names)\n  edata &lt;- read_html(URL)\n  body_nodes &lt;- edata %&gt;%\n    html_node('body') %&gt;%\n    html_children()\n  \n  # LOCATION\n  LOCATION &lt;- body_nodes %&gt;% \n    xml_find_all(\"//span[contains(@itemprop, 'location')]\") %&gt;% \n    html_text()\n  \n  # Organization website\n  WEBSITE &lt;- body_nodes %&gt;% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %&gt;% \n    html_text() %&gt;% tibble::enframe() %&gt;% \n    filter(!startsWith(value,'@')) %&gt;% \n    select(value) %&gt;% as.character()\n  \n  # Organization Name\n  NAME &lt;- body_nodes %&gt;% \n    xml_find_all(\"//h1[contains(@class, 'h2 lh-condensed')]\") %&gt;% \n    html_text() %&gt;%\n    gsub(pattern = \"\\\\s+\",replacement = \" \") %&gt;% \n    stringr::str_trim()\n  \n  # Twitter handle\n  TWITTER &lt;- body_nodes %&gt;% \n    xml_find_all(\"//a[contains(@itemprop, 'url')]\") %&gt;% \n    html_text() %&gt;% tibble::enframe() %&gt;% \n    filter(startsWith(value,'@')) %&gt;% \n    select(value) %&gt;% as.character()\n  \n  # Metadata vector\n  VEC &lt;- c(github=org_names,name=NAME, website=WEBSITE,twitter=TWITTER)\n  return(VEC)\n}\n\n# Do call\nrc.metadata &lt;- do.call(rbind,lapply(rc.org, FUN=github_scrape))\n\n# Load scraped metadata\nmeta.df &lt;- as.data.frame(rc.metadata)\nmeta.df &lt;- meta.df[order(meta.df$github),]\n\n# Add GitHub hyperlinks\nghURL &lt;- paste0(\"https://github.com/\", meta.df$github)\nmeta.df$github &lt;-paste0(\"&lt;a href='\",ghURL,\"'\",\n                        ' target=\\\"_blank\\\"&gt;',\n                        meta.df$github,\"&lt;/a&gt;\")\n# Remove NA's\nmeta.df[meta.df == \"character(0)\"] &lt;- NA\n\n# Reorder columns\nmeta.df &lt;- subset(meta.df, select=c('github','name','twitter','website'))\n\n# Convert to DT\nmeta.dt &lt;- DT::datatable(meta.df,escape = F,rownames = F,\n                         options = list(pageLength = 100,\n                                        autoWidth = TRUE,\n                                        fixedColumns = list(leftColumns = 0)))\nmeta.dt"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Behind the keyboard ‚å®Ô∏è",
    "section": "",
    "text": "W. Rodriguez\n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n\n\n    \n      \n        William G. Rodriguez-Reillo\n        Computational Biologist & Consultant\n        \n        \n          \n            My research background is in Computer Science and Microbiology. I have extensive expertise working with large-scale genomic data. \n            Currently, I work at Harvard Medical School as a Research Computing Consultant. As a consultant, I work closely with researchers \n            from Harvard and Affiliates hospitals. The consultations range from ‚ÄúI collected new data. What can I do next?‚Äù to optimize an \n            established pipeline. Besides consultations, I help to develop documentation and training to effectively use High-Performance \n            Computing (HPC) clusters and R for statistical computing and graphics."
  },
  {
    "objectID": "posts/leaflet/index.html",
    "href": "posts/leaflet/index.html",
    "title": "OpenStreetMap on R + Quarto",
    "section": "",
    "text": "Create a map widget using the leaflet package on R\n\nlibrary(leaflet)\n\nm &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=-66.74041348869325, lat=18.468307266376055, popup=\"Mi Alma Materüê∫\")"
  },
  {
    "objectID": "posts/leaflet/index.html#leaflet-widget",
    "href": "posts/leaflet/index.html#leaflet-widget",
    "title": "OpenStreetMap on R + Quarto",
    "section": "",
    "text": "Create a map widget using the leaflet package on R\n\nlibrary(leaflet)\n\nm &lt;- leaflet() %&gt;%\n  addTiles() %&gt;%  # Add default OpenStreetMap map tiles\n  addMarkers(lng=-66.74041348869325, lat=18.468307266376055, popup=\"Mi Alma Materüê∫\")"
  },
  {
    "objectID": "posts/leaflet/index.html#mi-alma-mater",
    "href": "posts/leaflet/index.html#mi-alma-mater",
    "title": "OpenStreetMap on R + Quarto",
    "section": "Mi alma mater",
    "text": "Mi alma mater"
  }
]